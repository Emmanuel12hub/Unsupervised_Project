{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Group Project\n",
    "\n",
    "### Project Title: 2403PTDS_Unsupervised_Project\n",
    "Analysing News Articles Dataset\n",
    "#### Done By: Classification Project Team (K Ebrahim, J Sithole, J Maleka, S Tlhale, N Mhlophe & M Majola)\n",
    "\n",
    "Â© ExploreAI 2024\n",
    "\n",
    "---\n",
    "\n",
    "## Table of Contents\n",
    "\n",
    "<a href=#BC> Project Overview</a>\n",
    "\n",
    "<a href=#one>1. Dataset</a>\n",
    "\n",
    "<a href=#two>2. Packages</a>\n",
    "\n",
    "<a href=#three>3. Environment </a>\n",
    "\n",
    "<a href=#four>4. Data Cleaning and Filtering</a>\n",
    "\n",
    "<a href=#five>5. Exploratory Data Analysis (EDA)</a>\n",
    "\n",
    "<a href=#six>6. Model Building and Evaluation</a>\n",
    "\n",
    "<a href=#seven>7. MLFlow </a>\n",
    "\n",
    "<a href=#seven>7. Streamlit </a>\n",
    "\n",
    "<a href=#eight>8. Conclusion and Future Work</a>\n",
    "\n",
    "<a href=#nine>9. Team Members</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    " <a id=\"BC\"></a>\n",
    "## **Background Context**\n",
    "<a href=#cont>Back to Table of Contents</a>\n",
    "\n",
    "* **Project Overview:**\n",
    "\n",
    "  This project focuses on building a hybrid anime recommendation system by combining collaborative and content-based filtering techniques.\n",
    "  Leveraging     an anime dataset from myanimelist.net, the system will predict user preferences and recommend relevant anime titles.  A key aspect of    this project,     as it's framed as unsupervised, will involve exploring unsupervised learning techniques for aspects like user or item clustering,\n",
    "  which can enhance     the recommendation process.  This mimics how platforms like Netflix and others provide tailored recommendations.\n",
    "\n",
    "* **Purpose:** \n",
    "    * Personalized Recommendations: To provide users with personalized anime recommendations based on their viewing history and preferences.\n",
    "    * Content Discovery: To facilitate the discovery of new and relevant anime titles that users might not otherwise find.\n",
    "    * Improved User Experience: To enhance the user experience on anime platforms by streamlining the content selection process.\n",
    "    * Unsupervised Learning Exploration: To investigate the application of unsupervised learning methods (e.g., clustering, dimensionality reduction)\n",
    "      for improving recommendation system performance.\n",
    "    * Data Analysis: To analyze user behavior and preferences related to anime consumption, potentially revealing insights through unsupervised methods.\n",
    "\n",
    "\n",
    "* **Details:**\n",
    "\n",
    "  * Dataset: The project will use a dataset from myanimelist.net containing user ratings for a large collection of anime titles. Data preprocessing and     feature engineering will be crucial, especially considering the potential for unsupervised learning.\n",
    "    \n",
    "  * Collaborative Filtering: This approach will identify users with similar viewing habits. Unsupervised learning can be used here, for instance, to\n",
    "    cluster users based on their rating patterns before applying collaborative filtering.\n",
    "      \n",
    "  * Content-Based Filtering: This approach will analyze anime characteristics (genre, themes, studio, synopsis). Unsupervised learning can be applied       to extract relevant features or group similar anime titles.\n",
    " \n",
    "    \n",
    "  * Hybrid Approach: The project will combine collaborative and content-based filtering, possibly using unsupervised learning results to inform the\n",
    "    hybrid strategy.\n",
    " \n",
    "    \n",
    "   * Unsupervised Learning Component: This is the core of the \"unsupervised\" aspect. Techniques like K-Means clustering for user segmentation, or\n",
    "    PCA/t-  SNE for dimensionality reduction of anime features, will be explored.\n",
    " \n",
    "     \n",
    "   * Evaluation Metrics: Performance will be evaluated using metrics like precision, recall, F1-score, RMSE, and potentially metrics specific to\n",
    "     evaluating the quality of clusters or reduced dimensions.\n",
    " \n",
    "     \n",
    "   * Technology Stack: Python with libraries like Pandas, NumPy, Scikit-learn, Surprise, and potentially others for visualization or specific\n",
    "      unsupervised learning algorithms.\n",
    "\n",
    "    **1. Objective:**\n",
    "\n",
    "    * Data Preprocessing & Feature Engineering: Clean the data, handle missing values, and engineer features suitable for both supervised and\n",
    "     unsupervised learning.\n",
    "\n",
    "    * Unsupervised Learning Implementation: Apply unsupervised learning techniques (clustering, dimensionality reduction) to user ratings and/or anime        features.\n",
    " \n",
    "    * Model Development: Implement collaborative, content-based, and hybrid recommendation models, integrating results from unsupervised learning.\n",
    " \n",
    "    * Performance Evaluation: Evaluate the performance of the models, focusing on the impact of the unsupervised learning component.\n",
    " \n",
    "    * Optimization: Fine-tune models and unsupervised learning parameters to maximize recommendation accuracy.\n",
    " \n",
    "    * Recommendation Generation: Develop a system to generate personalized recommendations.\n",
    " \n",
    "    * Documentation & Reporting: Document the project thoroughly, including the rationale behind unsupervised learning choices, implementation details,       evaluation results, and insights.\n",
    "\n",
    "    **2. Deliverables:**\n",
    "\n",
    "    * Preprocessed Dataset: Cleaned and prepared dataset, including any features created specifically for unsupervised learning.\n",
    "    * Unsupervised Learning Models: Code implementing the chosen unsupervised learning techniques (e.g., clustering, dimensionality reduction).\n",
    "    * Recommendation Models: Code implementing the collaborative, content-based, and hybrid recommendation models.\n",
    "    * Evaluation Report: A report detailing the evaluation metrics and a comparative analysis of the different approaches, emphasizing the contribution       of unsupervised learning.\n",
    "    * Recommendation System: A working system (script, API, or simple application) to generate recommendations.\n",
    "    * Code Repository: A well-organized and documented repository containing all code, data, and reports.\n",
    "\n",
    "\n",
    "    **3. Expected Outcomes:**\n",
    "\n",
    "    * Functional Recommendation System: A working anime recommendation system that provides personalized recommendations.\n",
    "    * Demonstrated Use of Unsupervised Learning: A clear demonstration of how unsupervised learning techniques can be applied to enhance a\n",
    "      recommendation system.\n",
    "    * Performance Analysis: A thorough analysis of the impact of unsupervised learning on recommendation performance.\n",
    "    * Insights: Potential insights gained from the data through unsupervised learning, such as user segments or anime feature relationships.\n",
    "    * Documented Project: A well-documented and reproducible project."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "<a href=#two></a>\n",
    "## **Data Collection and Description**\n",
    "<a href=#cont>Back to Table of Contents</a>\n",
    "\n",
    "The dataset is comprised of news articles that need to be classified into categories based on their content, including Business, Technology, Sports, Education, and Entertainment. You can find both the train.csv and test.csv\n",
    "\n",
    "**Dataset Features:**\n",
    "\n",
    "**Headlines**      The headline or title of the news article.\n",
    "\n",
    "**Description**    A brief summary or description of the news article.\n",
    "\n",
    "**Content**        The full text content of the news article.\n",
    "\n",
    "**URL**            The URL link to the original source of the news article.\n",
    "\n",
    "**Category**      The category or topic of the news article (e.g., business, education, entertainment, sports, technology).\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "* **Purpose:** Describe how the data was collected and provide an overview of its characteristics.\n",
    "* **Details:** Mention sources of the data, the methods used for collection (e.g., APIs, web scraping, datasets from repositories), and a general description of the dataset including size, scope, and types of data available (e.g., numerical, categorical).\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* **Installing and Importing Packages**\n",
    "\n",
    "**Purpose:** Set up the Python environment with necessary libraries and tools.\n",
    "\n",
    "**Details:** List and import all the Python packages that will be used throughout the project such as Pandas for data manipulation, Matplotlib/Seaborn for visualization, scikit-learn for modeling, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pandas in c:\\users\\f5267366\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (2.2.3)\n",
      "Requirement already satisfied: matplotlib in c:\\users\\f5267366\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (3.10.0)\n",
      "Collecting seaborn\n",
      "  Downloading seaborn-0.13.2-py3-none-any.whl.metadata (5.4 kB)\n",
      "Requirement already satisfied: numpy in c:\\users\\f5267366\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (2.2.1)\n",
      "Requirement already satisfied: scikit-learn in c:\\users\\f5267366\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (1.6.0)\n",
      "Collecting nltk\n",
      "  Downloading nltk-3.9.1-py3-none-any.whl.metadata (2.9 kB)\n",
      "Collecting collections2\n",
      "  Downloading collections2-0.3.0.tar.gz (3.3 kB)\n",
      "  Installing build dependencies: started\n",
      "  Installing build dependencies: finished with status 'done'\n",
      "  Getting requirements to build wheel: started\n",
      "  Getting requirements to build wheel: finished with status 'done'\n",
      "  Preparing metadata (pyproject.toml): started\n",
      "  Preparing metadata (pyproject.toml): finished with status 'done'\n",
      "Collecting wordcloud\n",
      "  Downloading wordcloud-1.9.4-cp312-cp312-win_amd64.whl.metadata (3.5 kB)\n",
      "Collecting textblob\n",
      "  Downloading textblob-0.19.0-py3-none-any.whl.metadata (4.4 kB)\n",
      "Requirement already satisfied: joblib in c:\\users\\f5267366\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (1.4.2)\n",
      "Requirement already satisfied: mlflow in c:\\users\\f5267366\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (2.19.0)\n",
      "Requirement already satisfied: ipython in c:\\users\\f5267366\\appdata\\roaming\\python\\python312\\site-packages (8.32.0)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\f5267366\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from pandas) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\f5267366\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from pandas) (2024.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\users\\f5267366\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from pandas) (2024.2)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in c:\\users\\f5267366\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from matplotlib) (1.3.1)\n",
      "Requirement already satisfied: cycler>=0.10 in c:\\users\\f5267366\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from matplotlib) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in c:\\users\\f5267366\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from matplotlib) (4.55.3)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in c:\\users\\f5267366\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from matplotlib) (1.4.8)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\f5267366\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from matplotlib) (24.2)\n",
      "Requirement already satisfied: pillow>=8 in c:\\users\\f5267366\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from matplotlib) (11.1.0)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in c:\\users\\f5267366\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from matplotlib) (3.2.1)\n",
      "Requirement already satisfied: scipy>=1.6.0 in c:\\users\\f5267366\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from scikit-learn) (1.15.0)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in c:\\users\\f5267366\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from scikit-learn) (3.5.0)\n",
      "Requirement already satisfied: click in c:\\users\\f5267366\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from nltk) (8.1.8)\n",
      "Collecting regex>=2021.8.3 (from nltk)\n",
      "  Downloading regex-2024.11.6-cp312-cp312-win_amd64.whl.metadata (41 kB)\n",
      "     ---------------------------------------- 0.0/41.5 kB ? eta -:--:--\n",
      "     --------- ------------------------------ 10.2/41.5 kB ? eta -:--:--\n",
      "     -------------------------------------- 41.5/41.5 kB 496.9 kB/s eta 0:00:00\n",
      "Collecting tqdm (from nltk)\n",
      "  Downloading tqdm-4.67.1-py3-none-any.whl.metadata (57 kB)\n",
      "     ---------------------------------------- 0.0/57.7 kB ? eta -:--:--\n",
      "     --------------------- ------------------ 30.7/57.7 kB 1.3 MB/s eta 0:00:01\n",
      "     --------------------------------- ---- 51.2/57.7 kB 525.1 kB/s eta 0:00:01\n",
      "     -------------------------------------- 57.7/57.7 kB 504.6 kB/s eta 0:00:00\n",
      "Requirement already satisfied: mlflow-skinny==2.19.0 in c:\\users\\f5267366\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from mlflow) (2.19.0)\n",
      "Requirement already satisfied: Flask<4 in c:\\users\\f5267366\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from mlflow) (3.1.0)\n",
      "Requirement already satisfied: Jinja2<4,>=3.0 in c:\\users\\f5267366\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from mlflow) (3.1.5)\n",
      "Requirement already satisfied: alembic!=1.10.0,<2 in c:\\users\\f5267366\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from mlflow) (1.14.0)\n",
      "Requirement already satisfied: docker<8,>=4.0.0 in c:\\users\\f5267366\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from mlflow) (7.1.0)\n",
      "Requirement already satisfied: graphene<4 in c:\\users\\f5267366\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from mlflow) (3.4.3)\n",
      "Requirement already satisfied: markdown<4,>=3.3 in c:\\users\\f5267366\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from mlflow) (3.7)\n",
      "Requirement already satisfied: pyarrow<19,>=4.0.0 in c:\\users\\f5267366\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from mlflow) (18.1.0)\n",
      "Requirement already satisfied: sqlalchemy<3,>=1.4.0 in c:\\users\\f5267366\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from mlflow) (2.0.36)\n",
      "Requirement already satisfied: waitress<4 in c:\\users\\f5267366\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from mlflow) (3.0.2)\n",
      "Requirement already satisfied: cachetools<6,>=5.0.0 in c:\\users\\f5267366\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from mlflow-skinny==2.19.0->mlflow) (5.5.0)\n",
      "Requirement already satisfied: cloudpickle<4 in c:\\users\\f5267366\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from mlflow-skinny==2.19.0->mlflow) (3.1.0)\n",
      "Requirement already satisfied: databricks-sdk<1,>=0.20.0 in c:\\users\\f5267366\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from mlflow-skinny==2.19.0->mlflow) (0.40.0)\n",
      "Requirement already satisfied: gitpython<4,>=3.1.9 in c:\\users\\f5267366\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from mlflow-skinny==2.19.0->mlflow) (3.1.44)\n",
      "Requirement already satisfied: importlib_metadata!=4.7.0,<9,>=3.7.0 in c:\\users\\f5267366\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from mlflow-skinny==2.19.0->mlflow) (8.5.0)\n",
      "Requirement already satisfied: opentelemetry-api<3,>=1.9.0 in c:\\users\\f5267366\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from mlflow-skinny==2.19.0->mlflow) (1.29.0)\n",
      "Requirement already satisfied: opentelemetry-sdk<3,>=1.9.0 in c:\\users\\f5267366\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from mlflow-skinny==2.19.0->mlflow) (1.29.0)\n",
      "Requirement already satisfied: protobuf<6,>=3.12.0 in c:\\users\\f5267366\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from mlflow-skinny==2.19.0->mlflow) (5.29.2)\n",
      "Requirement already satisfied: pyyaml<7,>=5.1 in c:\\users\\f5267366\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from mlflow-skinny==2.19.0->mlflow) (6.0.2)\n",
      "Requirement already satisfied: requests<3,>=2.17.3 in c:\\users\\f5267366\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from mlflow-skinny==2.19.0->mlflow) (2.32.3)\n",
      "Requirement already satisfied: sqlparse<1,>=0.4.0 in c:\\users\\f5267366\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from mlflow-skinny==2.19.0->mlflow) (0.5.3)\n",
      "Requirement already satisfied: colorama in c:\\users\\f5267366\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from ipython) (0.4.6)\n",
      "Requirement already satisfied: decorator in c:\\users\\f5267366\\appdata\\roaming\\python\\python312\\site-packages (from ipython) (5.1.1)\n",
      "Requirement already satisfied: jedi>=0.16 in c:\\users\\f5267366\\appdata\\roaming\\python\\python312\\site-packages (from ipython) (0.19.2)\n",
      "Requirement already satisfied: matplotlib-inline in c:\\users\\f5267366\\appdata\\roaming\\python\\python312\\site-packages (from ipython) (0.1.7)\n",
      "Requirement already satisfied: prompt_toolkit<3.1.0,>=3.0.41 in c:\\users\\f5267366\\appdata\\roaming\\python\\python312\\site-packages (from ipython) (3.0.50)\n",
      "Requirement already satisfied: pygments>=2.4.0 in c:\\users\\f5267366\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from ipython) (2.19.0)\n",
      "Requirement already satisfied: stack_data in c:\\users\\f5267366\\appdata\\roaming\\python\\python312\\site-packages (from ipython) (0.6.3)\n",
      "Requirement already satisfied: traitlets>=5.13.0 in c:\\users\\f5267366\\appdata\\roaming\\python\\python312\\site-packages (from ipython) (5.14.3)\n",
      "Requirement already satisfied: Mako in c:\\users\\f5267366\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from alembic!=1.10.0,<2->mlflow) (1.3.8)\n",
      "Requirement already satisfied: typing-extensions>=4 in c:\\users\\f5267366\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from alembic!=1.10.0,<2->mlflow) (4.12.2)\n",
      "Requirement already satisfied: pywin32>=304 in c:\\users\\f5267366\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from docker<8,>=4.0.0->mlflow) (308)\n",
      "Requirement already satisfied: urllib3>=1.26.0 in c:\\users\\f5267366\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from docker<8,>=4.0.0->mlflow) (2.3.0)\n",
      "Requirement already satisfied: Werkzeug>=3.1 in c:\\users\\f5267366\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from Flask<4->mlflow) (3.1.3)\n",
      "Requirement already satisfied: itsdangerous>=2.2 in c:\\users\\f5267366\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from Flask<4->mlflow) (2.2.0)\n",
      "Requirement already satisfied: blinker>=1.9 in c:\\users\\f5267366\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from Flask<4->mlflow) (1.9.0)\n",
      "Requirement already satisfied: graphql-core<3.3,>=3.1 in c:\\users\\f5267366\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from graphene<4->mlflow) (3.2.5)\n",
      "Requirement already satisfied: graphql-relay<3.3,>=3.1 in c:\\users\\f5267366\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from graphene<4->mlflow) (3.2.0)\n",
      "Requirement already satisfied: parso<0.9.0,>=0.8.4 in c:\\users\\f5267366\\appdata\\roaming\\python\\python312\\site-packages (from jedi>=0.16->ipython) (0.8.4)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\f5267366\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from Jinja2<4,>=3.0->mlflow) (3.0.2)\n",
      "Requirement already satisfied: wcwidth in c:\\users\\f5267366\\appdata\\roaming\\python\\python312\\site-packages (from prompt_toolkit<3.1.0,>=3.0.41->ipython) (0.2.13)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\f5267366\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
      "Requirement already satisfied: greenlet!=0.4.17 in c:\\users\\f5267366\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from sqlalchemy<3,>=1.4.0->mlflow) (3.1.1)\n",
      "Requirement already satisfied: executing>=1.2.0 in c:\\users\\f5267366\\appdata\\roaming\\python\\python312\\site-packages (from stack_data->ipython) (2.2.0)\n",
      "Requirement already satisfied: asttokens>=2.1.0 in c:\\users\\f5267366\\appdata\\roaming\\python\\python312\\site-packages (from stack_data->ipython) (3.0.0)\n",
      "Requirement already satisfied: pure-eval in c:\\users\\f5267366\\appdata\\roaming\\python\\python312\\site-packages (from stack_data->ipython) (0.2.3)\n",
      "Requirement already satisfied: google-auth~=2.0 in c:\\users\\f5267366\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from databricks-sdk<1,>=0.20.0->mlflow-skinny==2.19.0->mlflow) (2.37.0)\n",
      "Requirement already satisfied: gitdb<5,>=4.0.1 in c:\\users\\f5267366\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from gitpython<4,>=3.1.9->mlflow-skinny==2.19.0->mlflow) (4.0.12)\n",
      "Requirement already satisfied: zipp>=3.20 in c:\\users\\f5267366\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from importlib_metadata!=4.7.0,<9,>=3.7.0->mlflow-skinny==2.19.0->mlflow) (3.21.0)\n",
      "Requirement already satisfied: deprecated>=1.2.6 in c:\\users\\f5267366\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from opentelemetry-api<3,>=1.9.0->mlflow-skinny==2.19.0->mlflow) (1.2.15)\n",
      "Requirement already satisfied: opentelemetry-semantic-conventions==0.50b0 in c:\\users\\f5267366\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from opentelemetry-sdk<3,>=1.9.0->mlflow-skinny==2.19.0->mlflow) (0.50b0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\f5267366\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from requests<3,>=2.17.3->mlflow-skinny==2.19.0->mlflow) (3.4.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\f5267366\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from requests<3,>=2.17.3->mlflow-skinny==2.19.0->mlflow) (3.10)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\f5267366\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from requests<3,>=2.17.3->mlflow-skinny==2.19.0->mlflow) (2024.12.14)\n",
      "Requirement already satisfied: wrapt<2,>=1.10 in c:\\users\\f5267366\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from deprecated>=1.2.6->opentelemetry-api<3,>=1.9.0->mlflow-skinny==2.19.0->mlflow) (1.17.0)\n",
      "Requirement already satisfied: smmap<6,>=3.0.1 in c:\\users\\f5267366\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from gitdb<5,>=4.0.1->gitpython<4,>=3.1.9->mlflow-skinny==2.19.0->mlflow) (5.0.2)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in c:\\users\\f5267366\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from google-auth~=2.0->databricks-sdk<1,>=0.20.0->mlflow-skinny==2.19.0->mlflow) (0.4.1)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in c:\\users\\f5267366\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from google-auth~=2.0->databricks-sdk<1,>=0.20.0->mlflow-skinny==2.19.0->mlflow) (4.9)\n",
      "Requirement already satisfied: pyasn1<0.7.0,>=0.4.6 in c:\\users\\f5267366\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from pyasn1-modules>=0.2.1->google-auth~=2.0->databricks-sdk<1,>=0.20.0->mlflow-skinny==2.19.0->mlflow) (0.6.1)\n",
      "Downloading seaborn-0.13.2-py3-none-any.whl (294 kB)\n",
      "   ---------------------------------------- 0.0/294.9 kB ? eta -:--:--\n",
      "   --------------------------- ------------ 204.8/294.9 kB 4.2 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 294.9/294.9 kB 3.6 MB/s eta 0:00:00\n",
      "Downloading nltk-3.9.1-py3-none-any.whl (1.5 MB)\n",
      "   ---------------------------------------- 0.0/1.5 MB ? eta -:--:--\n",
      "   ------------ --------------------------- 0.5/1.5 MB 9.8 MB/s eta 0:00:01\n",
      "   ---------------- ----------------------- 0.6/1.5 MB 9.7 MB/s eta 0:00:01\n",
      "   ----------------- ---------------------- 0.7/1.5 MB 6.0 MB/s eta 0:00:01\n",
      "   --------------------- ------------------ 0.8/1.5 MB 4.7 MB/s eta 0:00:01\n",
      "   --------------------------- ------------ 1.0/1.5 MB 5.1 MB/s eta 0:00:01\n",
      "   ----------------------------- ---------- 1.1/1.5 MB 4.5 MB/s eta 0:00:01\n",
      "   ----------------------------- ---------- 1.1/1.5 MB 4.5 MB/s eta 0:00:01\n",
      "   ----------------------------- ---------- 1.1/1.5 MB 4.5 MB/s eta 0:00:01\n",
      "   ----------------------------- ---------- 1.1/1.5 MB 4.5 MB/s eta 0:00:01\n",
      "   ----------------------------- ---------- 1.1/1.5 MB 4.5 MB/s eta 0:00:01\n",
      "   ----------------------------- ---------- 1.1/1.5 MB 4.5 MB/s eta 0:00:01\n",
      "   ----------------------------- ---------- 1.1/1.5 MB 4.5 MB/s eta 0:00:01\n",
      "   ----------------------------- ---------- 1.1/1.5 MB 4.5 MB/s eta 0:00:01\n",
      "   ----------------------------- ---------- 1.1/1.5 MB 4.5 MB/s eta 0:00:01\n",
      "   ----------------------------- ---------- 1.1/1.5 MB 4.5 MB/s eta 0:00:01\n",
      "   ----------------------------- ---------- 1.1/1.5 MB 4.5 MB/s eta 0:00:01\n",
      "   ------------------------------ --------- 1.1/1.5 MB 1.5 MB/s eta 0:00:01\n",
      "   ------------------------------- -------- 1.2/1.5 MB 1.5 MB/s eta 0:00:01\n",
      "   ------------------------------------ --- 1.4/1.5 MB 1.6 MB/s eta 0:00:01\n",
      "   -------------------------------------- - 1.5/1.5 MB 1.7 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 1.5/1.5 MB 1.6 MB/s eta 0:00:00\n",
      "Downloading wordcloud-1.9.4-cp312-cp312-win_amd64.whl (301 kB)\n",
      "   ---------------------------------------- 0.0/301.2 kB ? eta -:--:--\n",
      "   ---------------------------------------  297.0/301.2 kB 9.2 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 301.2/301.2 kB 4.6 MB/s eta 0:00:00\n",
      "Downloading textblob-0.19.0-py3-none-any.whl (624 kB)\n",
      "   ---------------------------------------- 0.0/624.3 kB ? eta -:--:--\n",
      "   ------------- -------------------------- 204.8/624.3 kB 4.2 MB/s eta 0:00:01\n",
      "   ------------------------------ --------- 471.0/624.3 kB 5.0 MB/s eta 0:00:01\n",
      "   -------------------------------------- - 604.2/624.3 kB 4.2 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 624.3/624.3 kB 3.6 MB/s eta 0:00:00\n",
      "Downloading regex-2024.11.6-cp312-cp312-win_amd64.whl (273 kB)\n",
      "   ---------------------------------------- 0.0/273.6 kB ? eta -:--:--\n",
      "   ------------- -------------------------- 92.2/273.6 kB 5.5 MB/s eta 0:00:01\n",
      "   ----------------- ---------------------- 122.9/273.6 kB 1.8 MB/s eta 0:00:01\n",
      "   ----------------- ---------------------- 122.9/273.6 kB 1.8 MB/s eta 0:00:01\n",
      "   ----------------- ---------------------- 122.9/273.6 kB 1.8 MB/s eta 0:00:01\n",
      "   ----------------- ---------------------- 122.9/273.6 kB 1.8 MB/s eta 0:00:01\n",
      "   ------------------- ------------------ 143.4/273.6 kB 502.3 kB/s eta 0:00:01\n",
      "   ------------------- ------------------ 143.4/273.6 kB 502.3 kB/s eta 0:00:01\n",
      "   ------------------- ------------------ 143.4/273.6 kB 502.3 kB/s eta 0:00:01\n",
      "   ------------------- ------------------ 143.4/273.6 kB 502.3 kB/s eta 0:00:01\n",
      "   ------------------- ------------------ 143.4/273.6 kB 502.3 kB/s eta 0:00:01\n",
      "   ------------------- ------------------ 143.4/273.6 kB 502.3 kB/s eta 0:00:01\n",
      "   ------------------- ------------------ 143.4/273.6 kB 502.3 kB/s eta 0:00:01\n",
      "   ---------------------- --------------- 163.8/273.6 kB 265.7 kB/s eta 0:00:01\n",
      "   ------------------------ ------------- 174.1/273.6 kB 262.1 kB/s eta 0:00:01\n",
      "   --------------------------- ---------- 194.6/273.6 kB 274.3 kB/s eta 0:00:01\n",
      "   ------------------------------------ - 266.2/273.6 kB 341.3 kB/s eta 0:00:01\n",
      "   ------------------------------------ - 266.2/273.6 kB 341.3 kB/s eta 0:00:01\n",
      "   -------------------------------------- 273.6/273.6 kB 312.4 kB/s eta 0:00:00\n",
      "Downloading tqdm-4.67.1-py3-none-any.whl (78 kB)\n",
      "   ---------------------------------------- 0.0/78.5 kB ? eta -:--:--\n",
      "   ------------------------------------ --- 71.7/78.5 kB ? eta -:--:--\n",
      "   ------------------------------------ --- 71.7/78.5 kB ? eta -:--:--\n",
      "   ---------------------------------------- 78.5/78.5 kB 546.4 kB/s eta 0:00:00\n",
      "Building wheels for collected packages: collections2\n",
      "  Building wheel for collections2 (pyproject.toml): started\n",
      "  Building wheel for collections2 (pyproject.toml): finished with status 'done'\n",
      "  Created wheel for collections2: filename=collections2-0.3.0-py3-none-any.whl size=5367 sha256=3ea300483b08e1132ac4b68cc995bbf1ae8f8543d853284eb80587c282ff46e6\n",
      "  Stored in directory: c:\\users\\f5267366\\appdata\\local\\pip\\cache\\wheels\\c2\\e6\\8a\\48d6b632207f6bb68691ee95980385d660e4bff95a30706937\n",
      "Successfully built collections2\n",
      "Installing collected packages: collections2, tqdm, regex, nltk, wordcloud, textblob, seaborn\n",
      "Successfully installed collections2-0.3.0 nltk-3.9.1 regex-2024.11.6 seaborn-0.13.2 textblob-0.19.0 tqdm-4.67.1 wordcloud-1.9.4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 24.0 -> 25.0\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "# Uncomment the next line to install required packages\n",
    "!pip install pandas matplotlib seaborn numpy scikit-learn nltk collections2 wordcloud textblob joblib mlflow ipython"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import pandas as pd    # For data manipulation and analysis.\n",
    "import matplotlib.pyplot as plt # For creating static, interactive, and animated visualizations\n",
    "import seaborn as sns  # Built on top of Matplotlib, Seaborn simplifies the creation of attractive and informative statistical graphics.\n",
    "import numpy as np  # For numerical computations.\n",
    "import re  # For regular expressions, used in text data preprocessing to search, match, and manipulate strings.\n",
    "import string # Provides constants like string.punctuation and utility functions for working with strings.\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer # Converts text data into numerical feature vectors using the Term Frequency-Inverse Document Frequency (TF-IDF) technique.\n",
    "from sklearn.model_selection import train_test_split # Splits the dataset into training and testing subsets.\n",
    "from sklearn.linear_model import LogisticRegression # Implements Logistic Regression, a commonly used supervised learning algorithm for classification tasks.\n",
    "from sklearn.naive_bayes import MultinomialNB # Implements the Multinomial Naive Bayes algorithm, often used for text classification.\n",
    "from sklearn.svm import SVC # Implements Support Vector Classifier (SVC), a powerful supervised learning model for classification.\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix # Provides functions to evaluate the performance of machine learning models\n",
    "from sklearn.model_selection import GridSearchCV # Performs hyperparameter tuning by evaluating a grid of parameter combinations using cross-validation.\n",
    "from sklearn.model_selection import cross_val_score # Computes cross-validated scores for assessing model performance.\n",
    "import joblib # Used for saving and loading machine learning models or large datasets efficiently.\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from collections import Counter\n",
    "from wordcloud import WordCloud\n",
    "from textblob import TextBlob\n",
    "import joblib\n",
    "import mlflow\n",
    "import mlflow.pyfunc\n",
    "from IPython.display import display\n",
    "import warnings\n",
    "# Hide all future warnings\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "<a href=#three></a>\n",
    "## **Loading Data**\n",
    "<a href=#cont>Back to Table of Contents</a>\n",
    "\n",
    "* **Purpose:** Load the data into the notebook for manipulation and analysis.\n",
    "* **Details:** Show the code used to load the data and display the first few rows to give a sense of what the raw data looks like.\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   user_id  anime_id  rating\n",
      "0        1     11617      10\n",
      "1        1     11757      10\n",
      "2        1     15451      10\n",
      "3        2     11771      10\n",
      "4        3        20       8\n",
      "   user_id  anime_id\n",
      "0    40763     21405\n",
      "1    68791     10504\n",
      "2    40487      1281\n",
      "3    55290       165\n",
      "4    72323     11111\n"
     ]
    }
   ],
   "source": [
    "# Load the datasets\n",
    "df_train = pd.read_csv('train.zip')\n",
    "df = pd.read_csv('anime.csv')\n",
    "df_test = pd.read_csv('test.csv')\n",
    "df_submit = pd.read_csv('submission.csv')\n",
    "\n",
    "# Display the first few rows of the train dataset\n",
    "print(df_train.head())\n",
    "\n",
    "# Display the first few rows of the test dataset\n",
    "print(df_test.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "<a href=#four></a>\n",
    "## **Data Cleaning and Filtering**\n",
    "<a href=#cont>Back to Table of Contents</a>\n",
    "\n",
    "* **Purpose:** Prepare the data for analysis by cleaning and filtering.\n",
    "* **Details:** Include steps for handling missing values, removing outliers, correcting errors, and possibly reducing the data (filtering based on certain criteria or features).\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "<a href=#five></a>\n",
    "## **Exploratory Data Analysis (EDA)**\n",
    "<a href=#cont>Back to Table of Contents</a>\n",
    "\n",
    "* **Purpose:** Explore and visualize the data to uncover patterns, trends, and relationships.\n",
    "* **Details:** Use statistics and visualizations to explore the data. This may include histograms, box plots, scatter plots, and correlation matrices. Discuss any significant findings.\n",
    "---\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for missing values in the train dataset\n",
    "\n",
    "# Check for missing values in the test dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Basic statistics of the train and test dataset\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "<a href=#nine></a>\n",
    "## **Model Building and Evaluation**\n",
    "<a href=#cont>Back to Table of Contents</a>\n",
    "\n",
    "* **Purpose:** Summarize the findings and discuss future directions.\n",
    "* **Details:** Conclude with a summary of the results, insights gained, limitations of the study, and suggestions for future projects or improvements in methodology or data collection.\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# \n",
    "**1. Text Preprocessing:**\n",
    "\n",
    "* Replace the __Content__ column with the __Cleaned_Content__ column in the train_df.\n",
    "* Drop the __Category__ column in the test_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenization and Vectorization using TF-IDF\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data ssssssss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Final Model Evaluation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform cross-validation on the final SVM model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the final model on the test set\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the final model for deployment\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deploy the Model\n",
    "\n",
    "## a. Set Up the Environment\n",
    "Ensure that the deployment environment has the necessary dependencies and configurations. This includes:\n",
    "\n",
    "- Python environment (e.g., virtualenv or conda)\n",
    "- Required libraries (e.g., scikit-learn, joblib, pandas, numpy)\n",
    "\n",
    "You can create a `requirements.txt` file to list all the dependencies:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the model and vectorizer.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MLFLOW"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "<a href=#nine></a>\n",
    "## **Conclusion and Future Work**\n",
    "<a href=#cont>Back to Table of Contents</a>\n",
    "\n",
    "* **Purpose:** Summarize the findings and discuss future directions.\n",
    "* **Details:** Conclude with a summary of the results, insights gained, limitations of the study, and suggestions for future projects or improvements in methodology or data collection.\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Please use code cells to code in and do not forget to comment your code."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Additional Sections to Consider\n",
    "\n",
    "* ### Appendix: \n",
    "For any additional code, detailed tables, or extended data visualizations that are supplementary to the main content.\n",
    "\n",
    "* ### Collaborators: \n",
    "  - Emmanuel Majola\n",
    "  - Selogile Tlhale\n",
    "  - Josia Sithole\n",
    "  - Kyle Ebrahim\n",
    "  - Jerry Maleka\n",
    "  - Nhlokomo Mhlophe\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
