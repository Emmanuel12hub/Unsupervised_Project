{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Group Project\n",
    "\n",
    "### Project Title: 2403PTDS_Unsupervised_Project\n",
    "Analysing News Articles Dataset\n",
    "#### Done By: Classification Project Team (K Ebrahim, J Sithole, J Maleka, S Tlhale, N Mhlophe & M Majola)\n",
    "\n",
    "Â© ExploreAI 2024\n",
    "\n",
    "---\n",
    "\n",
    "## Table of Contents\n",
    "\n",
    "<a href=#BC> Project Overview</a>\n",
    "\n",
    "<a href=#one>1. Dataset</a>\n",
    "\n",
    "<a href=#two>2. Packages</a>\n",
    "\n",
    "<a href=#three>3. Environment </a>\n",
    "\n",
    "<a href=#four>4. Data Cleaning and Filtering</a>\n",
    "\n",
    "<a href=#five>5. Exploratory Data Analysis (EDA)</a>\n",
    "\n",
    "<a href=#six>6. Model Building and Evaluation</a>\n",
    "\n",
    "<a href=#seven>7. MLFlow </a>\n",
    "\n",
    "<a href=#seven>7. Streamlit </a>\n",
    "\n",
    "<a href=#eight>8. Conclusion and Future Work</a>\n",
    "\n",
    "<a href=#nine>9. Team Members</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    " <a id=\"BC\"></a>\n",
    "## **Background Context**\n",
    "<a href=#cont>Back to Table of Contents</a>\n",
    "\n",
    "* **Project Overview:**\n",
    "\n",
    "  This project focuses on building a hybrid anime recommendation system by combining collaborative and content-based filtering techniques.\n",
    "  Leveraging     an anime dataset from myanimelist.net, the system will predict user preferences and recommend relevant anime titles.  A key aspect of    this project,     as it's framed as unsupervised, will involve exploring unsupervised learning techniques for aspects like user or item clustering,\n",
    "  which can enhance     the recommendation process.  This mimics how platforms like Netflix and others provide tailored recommendations.\n",
    "\n",
    "* **Purpose:** \n",
    "    * Personalized Recommendations: To provide users with personalized anime recommendations based on their viewing history and preferences.\n",
    "    * Content Discovery: To facilitate the discovery of new and relevant anime titles that users might not otherwise find.\n",
    "    * Improved User Experience: To enhance the user experience on anime platforms by streamlining the content selection process.\n",
    "    * Unsupervised Learning Exploration: To investigate the application of unsupervised learning methods (e.g., clustering, dimensionality reduction)\n",
    "      for improving recommendation system performance.\n",
    "    * Data Analysis: To analyze user behavior and preferences related to anime consumption, potentially revealing insights through unsupervised methods.\n",
    "\n",
    "\n",
    "* **Details:**\n",
    "\n",
    "  * Dataset: The project will use a dataset from myanimelist.net containing user ratings for a large collection of anime titles. Data preprocessing and     feature engineering will be crucial, especially considering the potential for unsupervised learning.\n",
    "    \n",
    "  * Collaborative Filtering: This approach will identify users with similar viewing habits. Unsupervised learning can be used here, for instance, to\n",
    "    cluster users based on their rating patterns before applying collaborative filtering.\n",
    "      \n",
    "  * Content-Based Filtering: This approach will analyze anime characteristics (genre, themes, studio, synopsis). Unsupervised learning can be applied       to extract relevant features or group similar anime titles.\n",
    " \n",
    "    \n",
    "  * Hybrid Approach: The project will combine collaborative and content-based filtering, possibly using unsupervised learning results to inform the\n",
    "    hybrid strategy.\n",
    " \n",
    "    \n",
    "   * Unsupervised Learning Component: This is the core of the \"unsupervised\" aspect. Techniques like K-Means clustering for user segmentation, or\n",
    "    PCA/t-  SNE for dimensionality reduction of anime features, will be explored.\n",
    " \n",
    "     \n",
    "   * Evaluation Metrics: Performance will be evaluated using metrics like precision, recall, F1-score, RMSE, and potentially metrics specific to\n",
    "     evaluating the quality of clusters or reduced dimensions.\n",
    " \n",
    "     \n",
    "   * Technology Stack: Python with libraries like Pandas, NumPy, Scikit-learn, Surprise, and potentially others for visualization or specific\n",
    "      unsupervised learning algorithms.\n",
    "\n",
    "    **1. Objective:**\n",
    "\n",
    "    * Data Preprocessing & Feature Engineering: Clean the data, handle missing values, and engineer features suitable for both supervised and\n",
    "     unsupervised learning.\n",
    "\n",
    "    * Unsupervised Learning Implementation: Apply unsupervised learning techniques (clustering, dimensionality reduction) to user ratings and/or anime        features.\n",
    " \n",
    "    * Model Development: Implement collaborative, content-based, and hybrid recommendation models, integrating results from unsupervised learning.\n",
    " \n",
    "    * Performance Evaluation: Evaluate the performance of the models, focusing on the impact of the unsupervised learning component.\n",
    " \n",
    "    * Optimization: Fine-tune models and unsupervised learning parameters to maximize recommendation accuracy.\n",
    " \n",
    "    * Recommendation Generation: Develop a system to generate personalized recommendations.\n",
    " \n",
    "    * Documentation & Reporting: Document the project thoroughly, including the rationale behind unsupervised learning choices, implementation details,       evaluation results, and insights.\n",
    "\n",
    "    **2. Deliverables:**\n",
    "\n",
    "    * Preprocessed Dataset: Cleaned and prepared dataset, including any features created specifically for unsupervised learning.\n",
    "    * Unsupervised Learning Models: Code implementing the chosen unsupervised learning techniques (e.g., clustering, dimensionality reduction).\n",
    "    * Recommendation Models: Code implementing the collaborative, content-based, and hybrid recommendation models.\n",
    "    * Evaluation Report: A report detailing the evaluation metrics and a comparative analysis of the different approaches, emphasizing the contribution       of unsupervised learning.\n",
    "    * Recommendation System: A working system (script, API, or simple application) to generate recommendations.\n",
    "    * Code Repository: A well-organized and documented repository containing all code, data, and reports.\n",
    "\n",
    "\n",
    "    **3. Expected Outcomes:**\n",
    "\n",
    "    * Functional Recommendation System: A working anime recommendation system that provides personalized recommendations.\n",
    "    * Demonstrated Use of Unsupervised Learning: A clear demonstration of how unsupervised learning techniques can be applied to enhance a\n",
    "      recommendation system.\n",
    "    * Performance Analysis: A thorough analysis of the impact of unsupervised learning on recommendation performance.\n",
    "    * Insights: Potential insights gained from the data through unsupervised learning, such as user segments or anime feature relationships.\n",
    "    * Documented Project: A well-documented and reproducible project."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "<a href=#two></a>\n",
    "## **Data Collection and Description**\n",
    "<a href=#cont>Back to Table of Contents</a>\n",
    "\n",
    "The dataset is comprised of news articles that need to be classified into categories based on their content, including Business, Technology, Sports, Education, and Entertainment. You can find both the train.csv and test.csv\n",
    "\n",
    "**Dataset Features:**\n",
    "\n",
    "**Headlines**      The headline or title of the news article.\n",
    "\n",
    "**Description**    A brief summary or description of the news article.\n",
    "\n",
    "**Content**        The full text content of the news article.\n",
    "\n",
    "**URL**            The URL link to the original source of the news article.\n",
    "\n",
    "**Category**      The category or topic of the news article (e.g., business, education, entertainment, sports, technology).\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "* **Purpose:** Describe how the data was collected and provide an overview of its characteristics.\n",
    "* **Details:** Mention sources of the data, the methods used for collection (e.g., APIs, web scraping, datasets from repositories), and a general description of the dataset including size, scope, and types of data available (e.g., numerical, categorical).\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* **Installing and Importing Packages**\n",
    "\n",
    "**Purpose:** Set up the Python environment with necessary libraries and tools.\n",
    "\n",
    "**Details:** List and import all the Python packages that will be used throughout the project such as Pandas for data manipulation, Matplotlib/Seaborn for visualization, scikit-learn for modeling, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'wordcloud'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 20\u001b[0m\n\u001b[0;32m     18\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mnltk\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mstem\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m WordNetLemmatizer\n\u001b[0;32m     19\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mcollections\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Counter\n\u001b[1;32m---> 20\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mwordcloud\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m WordCloud\n\u001b[0;32m     21\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtextblob\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m TextBlob\n\u001b[0;32m     22\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mjoblib\u001b[39;00m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'wordcloud'"
     ]
    }
   ],
   "source": [
    "\n",
    "import pandas as pd    # For data manipulation and analysis.\n",
    "import matplotlib.pyplot as plt # For creating static, interactive, and animated visualizations\n",
    "import seaborn as sns  # Built on top of Matplotlib, Seaborn simplifies the creation of attractive and informative statistical graphics.\n",
    "import numpy as np  # For numerical computations.\n",
    "import re  # For regular expressions, used in text data preprocessing to search, match, and manipulate strings.\n",
    "import string # Provides constants like string.punctuation and utility functions for working with strings.\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer # Converts text data into numerical feature vectors using the Term Frequency-Inverse Document Frequency (TF-IDF) technique.\n",
    "from sklearn.model_selection import train_test_split # Splits the dataset into training and testing subsets.\n",
    "from sklearn.linear_model import LogisticRegression # Implements Logistic Regression, a commonly used supervised learning algorithm for classification tasks.\n",
    "from sklearn.naive_bayes import MultinomialNB # Implements the Multinomial Naive Bayes algorithm, often used for text classification.\n",
    "from sklearn.svm import SVC # Implements Support Vector Classifier (SVC), a powerful supervised learning model for classification.\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix # Provides functions to evaluate the performance of machine learning models\n",
    "from sklearn.model_selection import GridSearchCV # Performs hyperparameter tuning by evaluating a grid of parameter combinations using cross-validation.\n",
    "from sklearn.model_selection import cross_val_score # Computes cross-validated scores for assessing model performance.\n",
    "import joblib # Used for saving and loading machine learning models or large datasets efficiently.\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from collections import Counter\n",
    "from wordcloud import WordCloud\n",
    "from textblob import TextBlob\n",
    "import joblib\n",
    "import mlflow\n",
    "import mlflow.pyfunc\n",
    "from IPython.display import display\n",
    "import warnings\n",
    "# Hide all future warnings\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "<a href=#three></a>\n",
    "## **Loading Data**\n",
    "<a href=#cont>Back to Table of Contents</a>\n",
    "\n",
    "* **Purpose:** Load the data into the notebook for manipulation and analysis.\n",
    "* **Details:** Show the code used to load the data and display the first few rows to give a sense of what the raw data looks like.\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the datasets\n",
    "\n",
    "\n",
    "# Display the first few rows of the train dataset\n",
    "\n",
    "\n",
    "# Display the first few rows of the test dataset\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* **Test**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "<a href=#four></a>\n",
    "## **Data Cleaning and Filtering**\n",
    "<a href=#cont>Back to Table of Contents</a>\n",
    "\n",
    "* **Purpose:** Prepare the data for analysis by cleaning and filtering.\n",
    "* **Details:** Include steps for handling missing values, removing outliers, correcting errors, and possibly reducing the data (filtering based on certain criteria or features).\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "<a href=#five></a>\n",
    "## **Exploratory Data Analysis (EDA)**\n",
    "<a href=#cont>Back to Table of Contents</a>\n",
    "\n",
    "* **Purpose:** Explore and visualize the data to uncover patterns, trends, and relationships.\n",
    "* **Details:** Use statistics and visualizations to explore the data. This may include histograms, box plots, scatter plots, and correlation matrices. Discuss any significant findings.\n",
    "---\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for missing values in the train dataset\n",
    "\n",
    "# Check for missing values in the test dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Basic statistics of the train and test dataset\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "<a href=#nine></a>\n",
    "## **Model Building and Evaluation**\n",
    "<a href=#cont>Back to Table of Contents</a>\n",
    "\n",
    "* **Purpose:** Summarize the findings and discuss future directions.\n",
    "* **Details:** Conclude with a summary of the results, insights gained, limitations of the study, and suggestions for future projects or improvements in methodology or data collection.\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# \n",
    "**1. Text Preprocessing:**\n",
    "\n",
    "* Replace the __Content__ column with the __Cleaned_Content__ column in the train_df.\n",
    "* Drop the __Category__ column in the test_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenization and Vectorization using TF-IDF\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data ssssssss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Final Model Evaluation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform cross-validation on the final SVM model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the final model on the test set\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the final model for deployment\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deploy the Model\n",
    "\n",
    "## a. Set Up the Environment\n",
    "Ensure that the deployment environment has the necessary dependencies and configurations. This includes:\n",
    "\n",
    "- Python environment (e.g., virtualenv or conda)\n",
    "- Required libraries (e.g., scikit-learn, joblib, pandas, numpy)\n",
    "\n",
    "You can create a `requirements.txt` file to list all the dependencies:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the model and vectorizer.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MLFLOW"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "<a href=#nine></a>\n",
    "## **Conclusion and Future Work**\n",
    "<a href=#cont>Back to Table of Contents</a>\n",
    "\n",
    "* **Purpose:** Summarize the findings and discuss future directions.\n",
    "* **Details:** Conclude with a summary of the results, insights gained, limitations of the study, and suggestions for future projects or improvements in methodology or data collection.\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Please use code cells to code in and do not forget to comment your code."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Additional Sections to Consider\n",
    "\n",
    "* ### Appendix: \n",
    "For any additional code, detailed tables, or extended data visualizations that are supplementary to the main content.\n",
    "\n",
    "* ### Collaborators: \n",
    "  - Emmanuel Majola\n",
    "  - Selogile Tlhale\n",
    "  - Josia Sithole\n",
    "  - Kyle Ebrahim\n",
    "  - Jerry Maleka\n",
    "  - Nhlokomo Mhlophe\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
